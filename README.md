# Nome do projeto
Avaliação Empírica da Qualidade da Documentação de Código Gerada por Ferramentas de IA

Este projeto visa investigar empiricamente a qualidade da documentação de código gerada por ferramentas de Inteligência Artificial, como GitHub Copilot, ChatGPT e Amazon CodeWhisperer. O estudo utiliza métricas automáticas (BLEU, ROUGE, METEOR, CodeBLEU) e ferramentas de análise estática (SonarQube, ESLint) para avaliar aspectos de completude, conformidade e manutenibilidade da documentação gerada. Espera-se identificar padrões de qualidade, limitações das ferramentas avaliadas, e propor diretrizes práticas para o uso dessas soluções em projetos de Engenharia de Software.

## Alunos integrantes da equipe

* Walter Roberto Rodrigues Loubak

## Professores responsáveis

* Danilo Maia  (TCCI)
* Leonardo Vilela (TCCI)
* Raphael Ramos (TCCI)
* Cleiton Tavares (TCCI)
* Nome do orientador de TCC II (a ser definido)

## Instruções de Replicação/Reprodução
1. Seleção de uma base de código JavaScript composta por funções e classes extraídas de repositórios públicos no GitHub.

2. Para cada trecho de código, gere documentação utilizando GitHub Copilot, ChatGPT e Amazon CodeWhisperer, com prompts padronizados para JSDoc.

3. Avalie a documentação gerada usando métricas automáticas (BLEU, ROUGE, METEOR, CodeBLEU) e análise estática através das ferramentas SonarQube e ESLint.

4. Compare os resultados com a documentação manual selecionada como referência.

5. Registre todos os dados, scripts e configurações em um repositório aberto para garantir transparência e reprodutibilidade.
