# BLEU: A Method for Automatic Evaluation of Machine Translation

Papineni, Kishore; Roukos, Salim; Ward, Todd; Zhu, Wei-Jing. "BLEU: A method for automatic evaluation of machine translation," Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318, 2002.

üîó DOI: [10.3115/1073083.1073135](https://doi.org/10.3115/1073083.1073135)  
üèÜ **NAACL 2018 Test-of-Time Award**

## 1. Fichamento de Conte√∫do

Este artigo seminal prop√µe o BLEU (Bilingual Evaluation Understudy), um m√©todo autom√°tico revolucion√°rio para avalia√ß√£o de tradu√ß√£o autom√°tica que √© r√°pido, econ√¥mico e independente de idioma. A m√©trica BLEU correlaciona-se altamente com avalia√ß√µes humanas e tem custo marginal m√≠nimo por execu√ß√£o. A metodologia baseia-se em comparar n-gramas da tradu√ß√£o candidata com n-gramas de tradu√ß√µes de refer√™ncia humanas, calculando precis√£o modificada para evitar pontua√ß√µes inflacionadas. O m√©todo inclui uma penalidade de brevidade para evitar que tradu√ß√µes excessivamente curtas recebam pontua√ß√µes artificialmente altas. Os autores demonstram que a correla√ß√£o do BLEU com julgamentos humanos em n√≠vel de corpus √© compar√°vel √† correla√ß√£o entre dois avaliadores humanos independentes. O BLEU tornou-se o padr√£o de facto para avalia√ß√£o autom√°tica em tradu√ß√£o autom√°tica e foi posteriormente adaptado para outras tarefas de gera√ß√£o de linguagem natural, incluindo resumo autom√°tico e gera√ß√£o de c√≥digo. Os resultados validam que m√©tricas autom√°ticas podem servir como substitutos eficientes para ju√≠zes humanos quando h√° necessidade de avalia√ß√µes r√°pidas ou frequentes, democratizando o acesso √† avalia√ß√£o de sistemas de tradu√ß√£o autom√°tica.

## 2. Fichamento Bibliogr√°fico

* _BLEU Score_ (pontua√ß√£o BLEU) √© calculada como m√©dia geom√©trica ponderada de precis√µes de n-gramas modificadas, multiplicada por uma penalidade de brevidade exponencial (se√ß√£o 2).
* _Modified Precision_ (precis√£o modificada) previne que palavras sejam contadas m√∫ltiplas vezes al√©m de suas ocorr√™ncias nas tradu√ß√µes de refer√™ncia, evitando pontua√ß√µes inflacionadas (se√ß√£o 2.1).
* _Brevity Penalty_ (penalidade de brevidade) compensa tradu√ß√µes candidatas que s√£o mais curtas que suas tradu√ß√µes de refer√™ncia efetivas, impedindo que tradu√ß√µes muito breves recebam pontua√ß√µes altas (se√ß√£o 3).
* _N-gram Precision_ (precis√£o de n-gramas) mede a fra√ß√£o de n-gramas na tradu√ß√£o candidata que ocorrem em alguma tradu√ß√£o de refer√™ncia, com n tipicamente variando de 1 a 4 (metodologia).
* _Corpus-Level Correlation_ (correla√ß√£o em n√≠vel de corpus) avalia o qu√£o bem a m√©trica autom√°tica se alinha com julgamentos humanos agregados em m√∫ltiplas tradu√ß√µes (valida√ß√£o experimental).

## 3. Fichamento de Cita√ß√µes

* _"Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused."_
* _"We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."_
* _"The closer a machine translation is to a professional human translation, the better it is."_
* _"The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches."_
* _"BLEU's correlation with human judgment has been demonstrated to be as good as the correlation between two human judges."_
* _"The BLEU metric ranges from 0 to 1. Few translations will attain a score of 1 unless they are identical to a reference translation."_
