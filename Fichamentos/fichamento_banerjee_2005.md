# METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments

Banerjee, Satanjeev; Lavie, Alon. "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments," Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 65-72, 2005.

üîó ACL Anthology: [W05-0909](https://aclanthology.org/W05-0909/)

## 1. Fichamento de Conte√∫do

Este artigo apresenta METEOR (Metric for Evaluation of Translation with Explicit ORdering), uma m√©trica autom√°tica para avalia√ß√£o de tradu√ß√£o autom√°tica desenvolvida para abordar defici√™ncias percebidas no BLEU. A principal motiva√ß√£o √© melhorar a correla√ß√£o com julgamentos humanos, especialmente em n√≠vel de segmento (senten√ßa individual). A metodologia do METEOR baseia-se em alinhamento de unigramas entre tradu√ß√£o candidata e refer√™ncias, considerando correspond√™ncias exatas, variantes derivadas (stemming), sin√¥nimos e par√°frases atrav√©s do WordNet. Diferentemente do BLEU que enfatiza precis√£o, METEOR equilibra explicitamente precis√£o e recall atrav√©s de uma m√©dia harm√¥nica (F-measure), e inclui uma penalidade por fragmenta√ß√£o que favorece tradu√ß√µes com palavras consecutivas correspondentes. Os resultados demonstram que METEOR alcan√ßa correla√ß√£o de Pearson significativamente maior (r‚âà0.33-0.35) com avalia√ß√µes humanas no conjunto de dados TIDES 2003 comparado a outras m√©tricas. O estudo valida que recall desempenha papel mais importante que precis√£o isoladamente na obten√ß√£o de alta correla√ß√£o com julgamentos humanos. METEOR tornou-se amplamente utilizado na comunidade de processamento de linguagem natural e foi adaptado para diversas tarefas al√©m de tradu√ß√£o autom√°tica.

## 2. Fichamento Bibliogr√°fico

* _Unigram Matching_ (correspond√™ncia de unigramas) √© o processo de alinhar palavras individuais entre tradu√ß√£o candidata e refer√™ncias usando correspond√™ncias exatas, stemming e sin√¥nimos do WordNet (se√ß√£o 2).
* _Precision and Recall Balance_ (equil√≠brio entre precis√£o e recall) √© alcan√ßado atrav√©s do F-measure que combina ambas medidas, enfatizando recall atrav√©s de um par√¢metro de pondera√ß√£o (se√ß√£o 2.1).
* _Fragmentation Penalty_ (penalidade de fragmenta√ß√£o) reduz a pontua√ß√£o quando correspond√™ncias entre tradu√ß√£o e refer√™ncia ocorrem em chunks descont√≠nuos, favorecendo ordem de palavras similar (se√ß√£o 2.2).
* _Segment-Level Correlation_ (correla√ß√£o em n√≠vel de segmento) mede qu√£o bem a m√©trica correlaciona com julgamentos humanos para tradu√ß√µes individuais, n√£o apenas agregados de corpus (objetivo).
* _WordNet Synonyms_ (sin√¥nimos do WordNet) s√£o utilizados para identificar correspond√™ncias sem√¢nticas entre palavras que n√£o s√£o id√™nticas textualmente, melhorando a robustez da m√©trica (metodologia).

## 3. Fichamento de Cita√ß√µes

* _"We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations."_
* _"Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies."_
* _"Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation."_
* _"METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigram-precision, unigram-recall or the (arithmetic) F1 average of precision and recall."_
* _"The basic BLEU metric... does not adequately compensate for the lack of recall through its fixed brevity penalty."_
* _"Our experimental results strongly support the claim that recall plays a more important role than precision in obtaining high levels of correlation with human judgments."_
