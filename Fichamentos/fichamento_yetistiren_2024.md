# Evaluating the Code Quality of AI-Assisted Code Generation Tools

Yeti≈ütiren, Burak; √ñzsoy, I≈üƒ±k; Ayerdem, Miray; T√ºz√ºn, Eray. "Evaluating the code quality of AI-assisted code generation tools: An empirical study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT," IEEE Software, vol. 41, no. 3, pp. 86-94, 2024.

üîó arXiv: [2304.10778](https://arxiv.org/abs/2304.10778)

## 1. Fichamento de Conte√∫do

Este estudo realiza uma avalia√ß√£o emp√≠rica comparativa das ferramentas de gera√ß√£o de c√≥digo assistidas por IA mais proeminentes: GitHub Copilot, Amazon CodeWhisperer e ChatGPT. O objetivo √© comparar o desempenho dessas ferramentas em termos de m√©tricas de qualidade de c√≥digo, incluindo Validade do C√≥digo, Corre√ß√£o do C√≥digo, Seguran√ßa do C√≥digo, Confiabilidade do C√≥digo e Manutenibilidade do C√≥digo. A metodologia utiliza o benchmark HumanEval Dataset contendo 164 problemas de programa√ß√£o para avaliar as capacidades de gera√ß√£o de c√≥digo. O c√≥digo gerado √© ent√£o avaliado com base nas m√©tricas de qualidade propostas usando ferramentas como SonarQube. Os resultados revelam que as vers√µes mais recentes do ChatGPT, GitHub Copilot e Amazon CodeWhisperer geram c√≥digo correto 65,2%, 46,3% e 31,1% das vezes, respectivamente. As vers√µes mais recentes do GitHub Copilot e Amazon CodeWhisperer mostraram taxas de melhoria de 18% para GitHub Copilot e 7% para Amazon CodeWhisperer. A d√≠vida t√©cnica m√©dia, considerando code smells, foi de 8,9 minutos para ChatGPT, 9,1 minutos para GitHub Copilot e 5,6 minutos para Amazon CodeWhisperer. Este estudo destaca os pontos fortes e fracos de algumas das ferramentas de gera√ß√£o de c√≥digo mais populares, fornecendo insights valiosos para profissionais que buscam selecionar a ferramenta ideal para tarefas espec√≠ficas.

## 2. Fichamento Bibliogr√°fico

* _Code Validity_ (validade do c√≥digo) refere-se √† capacidade do c√≥digo gerado de ser compilado e executado sem erros sint√°ticos, medida pela taxa de sucesso na execu√ß√£o (se√ß√£o de metodologia).
* _Code Correctness_ (corre√ß√£o do c√≥digo) avalia se o c√≥digo gerado resolve corretamente o problema proposto, passando em todos os casos de teste fornecidos pelo HumanEval Dataset (se√ß√£o de m√©tricas).
* _Technical Debt_ (d√≠vida t√©cnica) representa o tempo estimado necess√°rio para corrigir problemas de qualidade identificados no c√≥digo, incluindo code smells, bugs e vulnerabilidades (se√ß√£o de avalia√ß√£o).
* _HumanEval Dataset_ √© um benchmark de 164 problemas de programa√ß√£o em Python usado para avaliar modelos de gera√ß√£o de c√≥digo, originalmente proposto pela OpenAI (se√ß√£o de setup experimental).
* _Code Security_ (seguran√ßa do c√≥digo) mede a presen√ßa de vulnerabilidades e falhas de seguran√ßa no c√≥digo gerado, avaliada usando an√°lise est√°tica com SonarQube (m√©tricas de qualidade).

## 3. Fichamento de Cita√ß√µes

* _"AI-assisted code generation tools have become increasingly prevalent in software engineering, offering the ability to generate code from natural language prompts or partial code inputs."_
* _"Our analysis reveals that the latest versions of ChatGPT, GitHub Copilot, and Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1% of the time, respectively."_
* _"The newer versions of GitHub Copilot and Amazon CodeWhisperer showed improvement rates of 18% for GitHub Copilot and 7% for Amazon CodeWhisperer."_
* _"The average technical debt, considering code smells, was found to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes for Amazon CodeWhisperer."_
* _"This study highlights the strengths and weaknesses of some of the most popular code generation tools, providing valuable insights for practitioners."_
* _"By comparing these generators, our results may assist practitioners in selecting the optimal tool for specific tasks, enhancing their decision-making process."_
